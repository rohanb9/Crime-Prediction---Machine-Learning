{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Project: Crime Prediction\n",
    " #### Name : Rohan Satishrao Borde\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "# PART1: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " '''\n",
    "    For this section as question mentioned, i will use the clean dataset to predict \n",
    "    whether the crime rate in a locality is greater than 0.1 per capita or not  load clean data\n",
    " '''   \n",
    "\n",
    "df = pd.read_csv(\"Crime Prediction Data\\\\communities-crime-clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Part1 a) Create a new field “highCrime” which is true if the crime rate per capita (ViolentCrimesPerPop) is greater than 0.1, and false otherwise. What are the percentage of positive and negative instances in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hcArray = []\n",
    "\n",
    "def create_hc(x):\n",
    "    if x['ViolentCrimesPerPop']>0.1:\n",
    "        hcArray.append(True)\n",
    "        return True\n",
    "    else:\n",
    "        hcArray.append(False)\n",
    "        return False\n",
    "    \n",
    "df['highCrime']=df.apply(create_hc,axis=1)\n",
    "\n",
    "def calculate_Percent(val):\n",
    "    cl=val.unique()\n",
    "    for c in cl:\n",
    "        T=(val.value_counts()[c]/val.count())*100\n",
    "        print(c,\" % instances ={0:.2f}\".format(T))\n",
    "\n",
    "calculate_Percent(df.highCrime)\n",
    "\n",
    "\n",
    "hcArray = pd.DataFrame(hcArray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Part1 b) Use DecisionTreeClassifier to learn a decision tree to predict highCrime on the entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here, non predictive attributes are removed \n",
    "df = df.drop([\"state\"], axis=1)\n",
    "df = df.drop([\"communityname\"], axis=1)\n",
    "df = df.drop([\"fold\"], axis=1)\n",
    "df = df.drop([\"highCrime\"], axis=1)\n",
    "df = df.drop([\"ViolentCrimesPerPop\"], axis=1)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Build Decision Tree Classifier \n",
    "DecTreeClassifier = tree.DecisionTreeClassifier()\n",
    "DecTreeClassifier = DecTreeClassifier.fit(df, hcArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Predict class vale for df\n",
    "DecTreeprediction = DecTreeClassifier.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sum(DecTreeprediction != hcArray[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### PArt1 b- i) What are the training accuracy, precision, and recall for this tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Accuracy, Precision and Recall \n",
    "\n",
    "print(\"Training Accuracy: \", metrics.accuracy_score(hcArray, DecTreeprediction))\n",
    "print(\"Training Precision: \", metrics.precision_score(hcArray, DecTreeprediction))\n",
    "print(\"Training Recall: \", metrics.recall_score(hcArray, DecTreeprediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### PArt1 b- ii) What are the main features used for classification? Can you explain why they make sense (or not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Features importance/ score: \")\n",
    "DecTreeClassifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"10 Most important features sorted by importance in descending order: \")\n",
    "\n",
    "impFeatures = sorted(zip(df.columns, DecTreeClassifier.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "print(impFeatures[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ==> The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. As per the above findings we can see the top 10 most important features based on their score. Feature with the higher score is having more importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part1 C) Now apply cross-validation (cross_val_score) to do 10-fold cross-validation to estimate the out-of-training accuracy of decision tree learning for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df,hcArray,test_size=0.1)\n",
    "DecTreeClassifier1 = tree.DecisionTreeClassifier()\n",
    "DecTreeClassifier1 = DecTreeClassifier1.fit(x_train, y_train)\n",
    "\n",
    "y_pred = DecTreeClassifier1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y_test,y_pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part1 C - i) What are the 10-fold cross-validation accuracy, precision, and recall?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DecTreeClassifier2 = tree.DecisionTreeClassifier()\n",
    "dt_cross_accuracy = cross_val_score(DecTreeClassifier2,df,hcArray[0].values,cv=10, scoring='accuracy')\n",
    "dt_cross_precision = cross_val_score(DecTreeClassifier2,df,hcArray[0].values,cv=10, scoring='precision')\n",
    "dt_cross_recall = cross_val_score(DecTreeClassifier2,df,hcArray[0].values,cv=10, scoring='recall')\n",
    "\n",
    "\n",
    "print(\"Cross validation Accuracies: \", dt_cross_accuracy )\n",
    "print(\"Accuracy Mean: \", np.mean(dt_cross_accuracy))\n",
    "\n",
    "print(\"\\n Cross validation Precision: \", dt_cross_precision)\n",
    "print(\"Precision Mean: \", np.mean(dt_cross_precision))\n",
    "\n",
    "print(\"\\n Cross validation Recall: \", dt_cross_recall)\n",
    "print(\"Recall Mean: \", np.mean(dt_cross_recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part1 C - ii) Why are they different from the results in the previous test?\n",
    "\n",
    "==>  Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it.\n",
    "Here, we are dividing the given data set into 10 sets and we perform the train and test of model on these sets.\n",
    "10 cross validation is basically used to reduce overfitting. The goal of cross validation is to limit problems like overfitting. Cross-validation combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance.\n",
    "So, There will be some difference in the above two results as Cross-validation gives more accurate answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# PART2: Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### \tUse GaussianNB to learn a Naive Bayes classifier to predict highCrime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df,hcArray,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gaussianClassifier = GaussianNB()\n",
    "gaussianClassifier = gaussianClassifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = gaussianClassifier.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y_test,y_pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part2 a - i) What is the 10-fold cross-validation accuracy, precision, and recall for this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gaussianClassifier1 = GaussianNB()\n",
    "\n",
    "gnb_cross_accuracy = cross_val_score(gaussianClassifier1,df,hcArray[0].values,cv=10, scoring='accuracy')\n",
    "gnb_cross_precision = cross_val_score(gaussianClassifier1,df,hcArray[0].values,cv=10, scoring='precision')\n",
    "gnb_cross_recall = cross_val_score(gaussianClassifier1,df,hcArray[0].values,cv=10, scoring='recall')\n",
    "\n",
    "\n",
    "print(\"Cross validation Accuracies: \", gnb_cross_accuracy )\n",
    "print(\"Accuracy Mean: \", np.mean(gnb_cross_accuracy))\n",
    "\n",
    "print(\"\\n Cross validation Precision: \", gnb_cross_precision)\n",
    "print(\"Precision Mean: \", np.mean(gnb_cross_precision))\n",
    "\n",
    "print(\"\\n Cross validation Recall: \", gnb_cross_recall)\n",
    "print(\"Recall Mean: \", np.mean(gnb_cross_recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part2 a - ii) What are the 10 most predictive features? This can be measured by the normalized absolute difference of means for the feature between the two classes. Why do these make sense (or not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This funtion evaluates the given formula in question about predictive features.\n",
    "\n",
    "def calculate(data,hcArray):\n",
    "    trueval = []\n",
    "    falseval = []\n",
    "    res=[]\n",
    "    for i in range(len(hcArray)):\n",
    "        if (hcArray[0].values[i]):\n",
    "            trueval.append(data[i])\n",
    "        else:\n",
    "            falseval.append(data[i])\n",
    "\n",
    "    return abs(np.mean(trueval)-np.mean(falseval))/ (np.std(trueval)+np.std(falseval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "predf=[]\n",
    "for i in df.columns:\n",
    "    predf.append(calculate(df[i],hcArray))\n",
    "\n",
    "impFeatures = sorted(zip(df.columns, predf), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"10 Most Predictive Features: \")\n",
    "impFeatures[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ==> Above are the 10 most predictive features. Here, We are calculating false as well as true values separately and we are taking difference from it. Larger the difference, more predictive feature. So, As difference increases the feature we get is more predictive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part2 a - iii) How do these results compare with your results from decision trees, above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> For the gaussianNB it's accuracy percentage is around 76 and for decision tree it's accuracy percentage is around 72. So, Based on accuracy score we can say that the gaussianNB have the more accuracy than the decision tree. \n",
    "Also, Precision mean for gaussianNB in case of the 10 cross validation is 91 percent and For the decsion tree it is of the 78 percent. So, The precision for the gaussianNB is more than the decision tree. \n",
    "Also, Recall for the gaussianNB is 69 percent and for the decision tree it is of 78 percent. So, recall for the decision tree is the better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part2 b) Use LinearSVC to learn a linear Support Vector Machine model to predict highCrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df,hcArray,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "linearsvcClassifier = LinearSVC()\n",
    "linearsvcClassifier = linearsvcClassifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = linearsvcClassifier.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred) )\n",
    "\n",
    "print(\"Precision: \", metrics.precision_score(y_test,y_pred))\n",
    "\n",
    "print(\"Recall: \", metrics.recall_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part2 b - i) What is the 10-fold cross-validation accuracy, precision, and recall for this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "linearsvcClassifier1 = LinearSVC()\n",
    "\n",
    "lsvc_cross_accuracy = cross_val_score(linearsvcClassifier1,df,hcArray[0].values,cv=10, scoring='accuracy')\n",
    "lsvc_cross_precision = cross_val_score(linearsvcClassifier1,df,hcArray[0].values,cv=10, scoring='precision')\n",
    "lsvc_cross_recall = cross_val_score(linearsvcClassifier1,df,hcArray[0].values,cv=10, scoring='recall')\n",
    "\n",
    "\n",
    "print(\"Cross validation Accuracies: \", lsvc_cross_accuracy )\n",
    "print(\"Accuracy Mean: \", np.mean(lsvc_cross_accuracy))\n",
    "\n",
    "print(\"\\n Cross validation Precision: \", lsvc_cross_precision)\n",
    "print(\"Precision Mean: \", np.mean(lsvc_cross_precision))\n",
    "\n",
    "print(\"\\n Cross validation Recall: \", lsvc_cross_recall)\n",
    "print(\"Recall Mean: \", np.mean(lsvc_cross_recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part2 b ii)\tWhat are the 10 most predictive features? This can be measured by the absolute feature weights in the model. Why do these make sense (or not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "linearsvcClassifier2 = LinearSVC()\n",
    "linearsvcClassifier2 =linearsvcClassifier2.fit(df,hcArray)\n",
    "coef  = abs(linearsvcClassifier2.coef_)\n",
    "impFeatures = sorted(zip(df.columns, coef[0]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "impFeatures[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Here, we are using absolute weights in the model. More the weight, more is the importance of perticular feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part2 b iii) How do these results compare with your results from decision trees, above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the linearSVC it's accuracy percentage is around 79 and for decision tree it's accuracy percentage is around 72. So, Based on accuracy score we can say that the linearSVC have the more accuracy than the decision tree. \n",
    "Also, Precision mean for linearSVC in case of the 10 cross validation is 84 percent and For the decsion tree it is of the 78 percent. So, The precision for the linearSVC is more than the decision tree. \n",
    "Also, Recall for the linearSVC is 83 percent and for the decision tree it is of 78 percent. So, recall for the linearSVC is the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART3: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 a) Use LinearRegression to learn a linear model directly predicting the crime rate per capita (ViolentCrimesPerPop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 a i) Using 10-fold cross-validation, what is the estimated mean-squared-error (MSE) of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linearR = LinearRegression()\n",
    "MSEforLR = cross_val_score(linearR, df, hcArray, scoring='neg_mean_squared_error', cv=10)\n",
    "print(\"MSE using 10 cross validation: \\n\", MSEforLR)\n",
    "\n",
    "print(\"Mean of above results: \", np.mean(MSEforLR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 a ii) What is the MSE on the training set (train on all the data then test on it all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linearR = LinearRegression()\n",
    "linearR.fit(df, hcArray)\n",
    "linearR_predict = linearR.predict(df)\n",
    "MSEforLR1 = metrics.mean_squared_error(hcArray, linearR_predict)\n",
    "\n",
    "print(\"Here, MSE is : \", MSEforLR1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linearR.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 a iii) What features are most predictive of a high crime rate? A low crime rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linearR_c=linearR.coef_[0].copy()\n",
    "linearR_c=list(map(lambda x:x,linearR_c))\n",
    "linearR_c2=list(map(lambda x:[x,linearR_c.index(x)],linearR_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linearR_c2_sorted=sorted(linearR_c2,key=lambda x:x[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most predictive of a high crime rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linearR_c2_sorted[-10:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low Crime rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linearR_c2_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 b) Use LinearRegression to learn a linear model directly predicting the crime rate per capita (ViolentCrimesPerPop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 b i) What is the estimated MSE of the model under 10-fold CV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridgeCV_M = RidgeCV(alphas= (10, 1, 0.1, 0.01,0.001))\n",
    "MSEFor_ridgeCV_M = cross_val_score(ridgeCV_M, df, hcArray, scoring='neg_mean_squared_error', cv=10)\n",
    "print(\"MSE for the Ridge under 10 fold :\", MSEFor_ridgeCV_M)\n",
    "print(\"Mean of above results: \", np.mean(MSEFor_ridgeCV_M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 b ii) What is the MSE on the training set (train on all the data then test on it all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridgeCV_M1 = RidgeCV(alphas= (10, 1, 0.1, 0.01,0.001))\n",
    "ridgeCV_M1 = ridgeCV_M1.fit(df, hcArray)\n",
    "ridgeCV_M1predict = ridgeCV_M1.predict(df)\n",
    "MSE_on_trainingForRidge = metrics.mean_squared_error(hcArray, ridgeCV_M1predict)\n",
    "print(\"MSE on the training set: \", MSE_on_trainingForRidge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 b iii) What is the best alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Here, Best alpha is \",ridgeCV_M1.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 b iv) What does this say about the amount of overfitting in linear regression for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Model complexity is inversely proportional to the alpha value. If alpha value is more then model is more complex. Also it is more probable to overfit.\n",
    "In linear regression, there are more chancees of underfitting. We use Ridge because it fits data more properly and it is less likely to overfit. So, in Ridge we could get proper results. As per the above results we can say that we are getting the best alpha as 1.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part3 c)\tNow use polynomial features to do quadratic (second-order) polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part3 c i) What is the estimated MSE of the model under 10-fold CV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "lr = LinearRegression()\n",
    "model = Pipeline([(\"polynomial_features\", pf),(\"linear_regression\", lr)])\n",
    "model.fit(df,hcArray)\n",
    "crossval_poly = cross_val_score(model,df,hcArray[0].values, scoring='neg_mean_squared_error',cv=10)\n",
    "\n",
    "print(\"CrossVal Results: \", crossval_poly)\n",
    "print(\"Mean of above results: \", np.mean(crossval_poly))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Part3 c ii)\tWhat is the MSE on the training set (train on all the data then test on it all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "lr = LinearRegression()\n",
    "model = Pipeline([(\"polynomial_features\", pf),(\"linear_regression\", lr)])\n",
    "model.fit(df, hcArray)\n",
    "y_pred = model.predict(df)\n",
    "metrics.mean_squared_error(hcArray, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Part3 c iii)\tDoes this mean the quadratic model is better than the linear model for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Quadratic model is always better than the linear model. Polynomial is always better than linear model. As in this our project crime data is very complex. For complex data more precise values get by the quadratic model. In polynomial we can use different degree of parameters and dimentions. We will be having different parameters in case of quadratic model. So, quadratic model will give better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dirty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load un-clean data\n",
    "\n",
    "dataframe = pd.read_csv(\"Crime Prediction Data\\\\communities-crime-full.csv\")\n",
    "\n",
    "# Removing the ? marks by NAN \n",
    "\n",
    "dataframe= dataframe.replace('?',np.nan)\n",
    "clean_df = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cleaned Data:\n",
    "\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hcArray1 = []\n",
    "\n",
    "def create_hc(x):\n",
    "    if x['ViolentCrimesPerPop']>0.1:\n",
    "        hcArray1.append(True)\n",
    "        return True\n",
    "    else:\n",
    "        hcArray1.append(False)\n",
    "        return False\n",
    "    \n",
    "clean_df['highCrime']=clean_df.apply(create_hc,axis=1)\n",
    "\n",
    "def calculate_Percent(val):\n",
    "    cl=val.unique()\n",
    "    for c in cl:\n",
    "        T=(val.value_counts()[c]/val.count())*100\n",
    "        print(c,\" % instances ={0:.2f}\".format(T))\n",
    "\n",
    "calculate_Percent(clean_df.highCrime)\n",
    "\n",
    "hcArray1 = pd.DataFrame(hcArray1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = clean_df\n",
    "hcArray = hcArray1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### DirtyData b) Use DecisionTreeClassifier to learn a decision tree to predict highCrime on the entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here, non predictive attributes are removed \n",
    "df = df.drop([\"state\"], axis=1)\n",
    "df = df.drop([\"communityname\"], axis=1)\n",
    "df = df.drop([\"fold\"], axis=1)\n",
    "df = df.drop([\"highCrime\"], axis=1)\n",
    "df = df.drop([\"ViolentCrimesPerPop\"], axis=1)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Build Decision Tree Classifier \n",
    "DecTreeClassifier = tree.DecisionTreeClassifier()\n",
    "DecTreeClassifier = DecTreeClassifier.fit(df, hcArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Predict class vale for df\n",
    "DecTreeprediction = DecTreeClassifier.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sum(DecTreeprediction != hcArray[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### DirtyData b- i) What are the training accuracy, precision, and recall for this tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accuracy, Precision and Recall \n",
    "\n",
    "print(\"Training Accuracy: \", metrics.accuracy_score(hcArray, DecTreeprediction))\n",
    "print(\"Training Precision: \", metrics.precision_score(hcArray, DecTreeprediction))\n",
    "print(\"Training Recall: \", metrics.recall_score(hcArray, DecTreeprediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### DirtyData b- ii) What are the main features used for classification? Can you explain why they make sense (or not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Features importance/ score: \")\n",
    "DecTreeClassifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"10 Most important features sorted by importance in descending order: \")\n",
    "\n",
    "impFeatures = sorted(zip(df.columns, DecTreeClassifier.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "print(impFeatures[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### DirtyData C) Now apply cross-validation (cross_val_score) to do 10-fold cross-validation to estimate the out-of-training accuracy of decision tree learning for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df,hcArray,test_size=0.1)\n",
    "DecTreeClassifier1 = tree.DecisionTreeClassifier()\n",
    "DecTreeClassifier1 = DecTreeClassifier1.fit(x_train, y_train)\n",
    "\n",
    "y_pred = DecTreeClassifier1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(y_test,y_pred))\n",
    "print(\"Recall: \", metrics.recall_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### DirtyData C - i) What are the 10-fold cross-validation accuracy, precision, and recall?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DecTreeClassifier2 = tree.DecisionTreeClassifier()\n",
    "dt_cross_accuracy = cross_val_score(DecTreeClassifier2,df,hcArray[0].values,cv=10, scoring='accuracy')\n",
    "dt_cross_precision = cross_val_score(DecTreeClassifier2,df,hcArray[0].values,cv=10, scoring='precision')\n",
    "dt_cross_recall = cross_val_score(DecTreeClassifier2,df,hcArray[0].values,cv=10, scoring='recall')\n",
    "\n",
    "\n",
    "print(\"Cross validation Accuracies: \", dt_cross_accuracy )\n",
    "print(\"Accuracy Mean: \", np.mean(dt_cross_accuracy))\n",
    "\n",
    "print(\"\\n Cross validation Precision: \", dt_cross_precision)\n",
    "print(\"Precision Mean: \", np.mean(dt_cross_precision))\n",
    "\n",
    "print(\"\\n Cross validation Recall: \", dt_cross_recall)\n",
    "print(\"Recall Mean: \", np.mean(dt_cross_recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Extra Credit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 5 a i.Experiment with two learning methods other than those described above (one can be a non-linear kernel for SVM) for the classification problem, explaining clearly what you did. Show CV results for both the clean and full datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean DATA\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "df = pd.read_csv(\"Crime Prediction Data\\\\communities-crime-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hcArray = []\n",
    "\n",
    "def create_hc(x):\n",
    "    if x['ViolentCrimesPerPop']>0.1:\n",
    "        hcArray.append(True)\n",
    "        return True\n",
    "    else:\n",
    "        hcArray.append(False)\n",
    "        return False\n",
    "    \n",
    "df['highCrime']=df.apply(create_hc,axis=1)\n",
    "\n",
    "def calculate_Percent(val):\n",
    "    cl=val.unique()\n",
    "    for c in cl:\n",
    "        T=(val.value_counts()[c]/val.count())*100\n",
    "        print(c,\" % instances ={0:.2f}\".format(T))\n",
    "\n",
    "calculate_Percent(df.highCrime)\n",
    "\n",
    "\n",
    "hcArray = pd.DataFrame(hcArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here, non predictive attributes are removed \n",
    "df = df.drop([\"state\"], axis=1)\n",
    "df = df.drop([\"communityname\"], axis=1)\n",
    "df = df.drop([\"fold\"], axis=1)\n",
    "df = df.drop([\"highCrime\"], axis=1)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "Modal_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "print(\"Accuracy: \", cross_val_score(Modal_svm,df,hcArray[0].values,cv=10, scoring='accuracy'))\n",
    "print(\"Precision: \", cross_val_score(Modal_svm,df,hcArray[0].values,cv=10, scoring='precision'))\n",
    "print(\"Recall: \",cross_val_score(Modal_svm,df,hcArray[0].values,cv=10, scoring='recall'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "df_full = pd.read_csv(\"Crime Prediction Data\\\\communities-crime-full.csv\")\n",
    "\n",
    "df_full = df_full.replace('?', np.nan) \n",
    "df_full = df_full.dropna()\n",
    "\n",
    "df = df_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hcArray = []\n",
    "\n",
    "def create_hc(x):\n",
    "    if x['ViolentCrimesPerPop']>0.1:\n",
    "        hcArray.append(True)\n",
    "        return True\n",
    "    else:\n",
    "        hcArray.append(False)\n",
    "        return False\n",
    "    \n",
    "df['highCrime']=df.apply(create_hc,axis=1)\n",
    "\n",
    "def calculate_Percent(val):\n",
    "    cl=val.unique()\n",
    "    for c in cl:\n",
    "        T=(val.value_counts()[c]/val.count())*100\n",
    "        print(c,\" % instances ={0:.2f}\".format(T))\n",
    "\n",
    "calculate_Percent(df.highCrime)\n",
    "\n",
    "\n",
    "hcArray = pd.DataFrame(hcArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here, non predictive attributes are removed \n",
    "df = df.drop([\"state\"], axis=1)\n",
    "df = df.drop([\"communityname\"], axis=1)\n",
    "df = df.drop([\"fold\"], axis=1)\n",
    "df = df.drop([\"highCrime\"], axis=1)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "Modal_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "print(\"Accuracy: \", cross_val_score(Modal_svm,df,hcArray[0].values,cv=10, scoring='accuracy'))\n",
    "print(\"Precision: \", cross_val_score(Modal_svm,df,hcArray[0].values,cv=10, scoring='precision'))\n",
    "print(\"Recall: \",cross_val_score(Modal_svm,df,hcArray[0].values,cv=10, scoring='recall'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
